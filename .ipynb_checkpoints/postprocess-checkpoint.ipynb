{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoid the border effect while evaluating\n",
    "\n",
    "- While the labelling process have kept only wheat with more than 30%, it was still an ambiguous task\n",
    "- To solve the ambiguity while evaluating algorithm for localization, two specifics preprocessing tasks have been applied to the original labels. We recommand to filter your JSON with the same procedure as us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Localization\n",
    "\n",
    "- To avoid the ambiguity of detection wheat head on the border, all boxes on the border are removed\n",
    "- A square of 921x921px is centered on the image. All boxes that don't have a complete overlap with this square are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np \n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_boxes(boxes,min_area=10):\n",
    "    area = boxes[:,2]*boxes[:,3]\n",
    "\n",
    "    return np.squeeze(np.argwhere(area > min_area))\n",
    "\n",
    "\n",
    "def filter_border(boxes, patch_size,sensitivity):\n",
    "    (x1_a,y1_a) = (((np.array(patch_size)*sensitivity)/2)-1).astype(int)\n",
    "    (x2_a, y2_a) = [x1_a, y1_a] + np.array(patch_size)*(1-sensitivity)\n",
    "    \n",
    "    boxes = np.array(boxes)\n",
    "    x = boxes[:,0]\n",
    "    y = boxes[:,1]\n",
    "    h = boxes[:,2]\n",
    "    w = boxes[:,3]\n",
    "\n",
    "    areas = h*w\n",
    "\n",
    "    xx = x+h\n",
    "    yy= w+y\n",
    "\n",
    "    # a réécrire\n",
    "    xx1 = np.maximum(x, x1_a)\n",
    "    yy1 = np.maximum(y, y1_a)\n",
    "    xx2 = np.minimum(xx, x2_a)\n",
    "    yy2 = np.minimum(yy, y2_a)\n",
    "\n",
    "    w = np.maximum(0, xx2 - xx1 + 1)\n",
    "    h = np.maximum(0, yy2 - yy1 + 1)\n",
    "    \n",
    "    overlap = (w * h) / areas\n",
    "    pick_boxes = np.argwhere(overlap != 1.)\n",
    "\n",
    "    return np.squeeze(pick_boxes)\n",
    "\n",
    "def clean_json(\n",
    "    data,\n",
    "    sensitivity=0.1\n",
    "    ):\n",
    "\n",
    "\n",
    "    new_ann = []\n",
    "    patch_size = (data[\"images\"][0][\"width\"],data[\"images\"][0][\"height\"])\n",
    "    \n",
    "    for img_ann in tqdm(data[\"images\"]):\n",
    "\n",
    "\n",
    "        temp_ann = np.array([ann for ann in data[\"annotations\"] if ann[\"image_id\"] == img_ann[\"id\"]])\n",
    "\n",
    "        boxes = np.array([np.array(ann[\"bbox\"])for ann in data[\"annotations\"] if ann[\"image_id\"] == img_ann[\"id\"]])\n",
    "        if len(boxes) > 1:\n",
    "\n",
    "            pick_1 = clean_boxes(boxes)\n",
    "\n",
    "            pick_2 = filter_border(boxes[pick_1], patch_size,sensitivity)\n",
    "            temp_ann = temp_ann[pick_1][pick_2]\n",
    "\n",
    "        new_ann += list(temp_ann)\n",
    "\n",
    "    data[\"annotations\"] = new_ann\n",
    "\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8b4e347f1546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsonp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/home/etdavid/Projects/1_research/3_wheat_counting/3-GWHD/biased-result/submit/{jsonp.name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7cbeccdda6a9>\u001b[0m in \u001b[0;36mclean_json\u001b[0;34m(data, sensitivity)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mnew_ann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mpatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"width\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"height\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimg_ann\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "reference_data = Path(\"/home/etdavid/Projects/1_research/3_wheat_counting/3-GWHD/biased-result/\")\n",
    "for jsonp in reference_data.glob(\"corrected*.json\"):\n",
    "\n",
    "    data = json.load(jsonp.open())\n",
    "    data = clean_json(data)\n",
    "\n",
    "    Path(f\"/home/etdavid/Projects/1_research/3_wheat_counting/3-GWHD/biased-result/submit/{jsonp.name}\").write_text(json.dumps(data))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting\n",
    "\n",
    "- To avoid border effect, counting is generated thank to the following procedure:\n",
    "    - A square of 921x921px is centered on the image\n",
    "    - All boxes strictly outside the square are removed\n",
    "    - Boxes completly within the square count for one\n",
    "    - Boxes that are cropped will weight the proportion of remaining pixel. For instance if a boxe with an area of 150 pixels is cropped to a box with an area of 75 pixels, it will be count as 75 / 150 = 0.5 in the total count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def weighted_count(boxes, patch_size,sensitivity):\n",
    "    \n",
    "    \n",
    "    (x1_a,y1_a) = (((np.array(patch_size)*sensitivity)/2)-1).astype(int)\n",
    "    (x2_a, y2_a) = [x1_a, y1_a] + np.array(patch_size)*(1-sensitivity)\n",
    "    \n",
    "    boxes = np.array(boxes)\n",
    "    x = boxes[:,0]\n",
    "    y = boxes[:,1]\n",
    "    h = boxes[:,2]\n",
    "    w = boxes[:,3]\n",
    "    \n",
    "\n",
    "    areas = h*w\n",
    "\n",
    "    xx = x+h\n",
    "    yy= w+y\n",
    "\n",
    "    xx1 = np.maximum(x, x1_a)\n",
    "    yy1 = np.maximum(y, y1_a)\n",
    "    xx2 = np.minimum(xx, x2_a)\n",
    "    yy2 = np.minimum(yy, y2_a)\n",
    "\n",
    "    hh = np.maximum(0, xx2 - xx1 + 1)\n",
    "    ww = np.maximum(0, yy2 - yy1 + 1)\n",
    "    t = w*h\n",
    "    \n",
    "    overlap = (ww * hh) / areas\n",
    "    overlap[overlap >1] =1\n",
    "\n",
    "\n",
    "    return np.sum(overlap)\n",
    "\n",
    "def generate_countcsv(\n",
    "    sessions_path,\n",
    "    out_name,\n",
    "    sensitivity=0.1\n",
    "    ):\n",
    "    \n",
    "    count_csv = []\n",
    "    \n",
    "    for sessp in sessions_path:\n",
    "        data = json.load(sessp.open())\n",
    "        session_name = sessp.with_suffix(\"\").name\n",
    "        \n",
    "        patch_size = (data[\"images\"][0][\"width\"],data[\"images\"][0][\"height\"])\n",
    "\n",
    "        for img_ann in tqdm(data[\"images\"]):\n",
    "            img_name = img_ann[\"file_name\"]\n",
    "\n",
    "            boxes = np.array([np.array(ann[\"bbox\"])for ann in data[\"annotations\"] if ann[\"image_id\"] == img_ann[\"id\"]])\n",
    "            if len(boxes) > 1:\n",
    "\n",
    "                pick_1 = clean_boxes(boxes)\n",
    "                \n",
    "                count = weighted_count(boxes[pick_1], patch_size,sensitivity)\n",
    "            else:\n",
    "                count  = 0\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "            count_csv.append([session_name, img_name, count, len(boxes)])\n",
    "\n",
    "    count_df = pd.DataFrame(count_csv,columns=[\"session\",\"image_name\",\"count\",\"control_count\"])\n",
    "\n",
    "    count_df.to_csv(out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f928924b4431482b81f90e3ab8e6e4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=142.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5bcfeb98714473be5c4ecfc8625e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=120.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f909a6d173349ffb69f1e717a9c3b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=994.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783794ec028f49fda1d7c682d146da0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reference_data = Path(\"complete_test_json\")\n",
    "sessions_path =list(reference_data.glob(\"*.json\"))\n",
    "\n",
    "generate_countcsv(sessions_path,\"/home/etdavid/Projects/2_modules_dev/global-wheat-codalab/bundle/reference_data/count.csv\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capte-detection-3]",
   "language": "python",
   "name": "conda-env-capte-detection-3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
