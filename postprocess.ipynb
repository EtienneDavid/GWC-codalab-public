{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing the data\n",
    "\n",
    "- Please keep in mind that step 1 and 2 are independant and should be apply to your _original_ json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0- Formatting of prediction COCO json\n",
    "\n",
    "For localization, competitors are required to follow the prediction json format defined by ms COCO: https://cocodataset.org/#format-results , which is a list of annotations, structured in a standard defined by COCO. For each domain, you need to generate a different JSON file called utokyo_1, utokyo_2, nau_1, uq_1. You need to assign each annotations to one image: please use the correspondance.csv file to retrieve the right image_id. You can find a sample in the github\n",
    "\n",
    "If you have predicted a full COCO JSON, you can just use the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sess in sessions_path:\n",
    "    data = json.load(sess.open())\n",
    "    data = data[\"annotations\"]\n",
    "    sess.write_text(json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Localization\n",
    "\n",
    "- To avoid the ambiguity of detection wheat head on the border, all boxes on the border are removed for the localization task. You are required to filter your solution with the following script in order to not overflow the result server :)\n",
    "- A square of 1004x1004px is centered on the image. All boxes that don't have a complete overlap with this square are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np \n",
    "from tqdm.notebook import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_boxes(boxes,min_area=10):\n",
    "    area = boxes[:,2]*boxes[:,3]\n",
    "\n",
    "    return np.squeeze(np.argwhere(area > min_area))\n",
    "\n",
    "\n",
    "def filter_border(boxes, patch_size,sensitivity):\n",
    "    (x1_a,y1_a) = (((np.array(patch_size)*sensitivity)/2)-1).astype(int)\n",
    "    (x2_a, y2_a) = [x1_a, y1_a] + np.array(patch_size)*(1-sensitivity)\n",
    "    \n",
    "    boxes = np.array(boxes)\n",
    "    x = boxes[:,0]\n",
    "    y = boxes[:,1]\n",
    "    h = boxes[:,2]\n",
    "    w = boxes[:,3]\n",
    "\n",
    "    areas = h*w\n",
    "\n",
    "    xx = x+h\n",
    "    yy= w+y\n",
    "\n",
    "    # a réécrire\n",
    "    xx1 = np.maximum(x, x1_a)\n",
    "    yy1 = np.maximum(y, y1_a)\n",
    "    xx2 = np.minimum(xx, x2_a)\n",
    "    yy2 = np.minimum(yy, y2_a)\n",
    "\n",
    "    w = np.maximum(0, xx2 - xx1 + 1)\n",
    "    h = np.maximum(0, yy2 - yy1 + 1)\n",
    "    \n",
    "    overlap = (w * h) / areas\n",
    "    overlap[overlap >1] = 1\n",
    "    \n",
    "    pick_boxes = np.argwhere(overlap == 1.)\n",
    "\n",
    "    return np.squeeze(pick_boxes)\n",
    "\n",
    "def clean_json(\n",
    "    annotations,\n",
    "    sensitivity=0.01,\n",
    "    patch_size=(1024,1024)\n",
    "    \n",
    "    ):\n",
    "\n",
    "\n",
    "    list_img = list(set([ann[\"image_id\"] for ann in annotations]))\n",
    "    new_ann = []\n",
    "    \n",
    "    for img_id in tqdm(list_img):\n",
    "\n",
    "\n",
    "        temp_ann = np.array([ann for ann in annotations if ann[\"image_id\"] == img_id])\n",
    "\n",
    "        boxes = np.array([np.array(ann[\"bbox\"])for ann in annotations if ann[\"image_id\"] == img_id])\n",
    "        if len(boxes) > 1:\n",
    "\n",
    "            pick_1 = clean_boxes(boxes)\n",
    "\n",
    "            pick_2 = filter_border(boxes[pick_1], patch_size,sensitivity)\n",
    "            temp_ann = temp_ann[pick_1][pick_2]\n",
    "\n",
    "\n",
    "        new_ann += list(temp_ann)\n",
    "\n",
    "\n",
    "    return(new_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uq_1\n",
      "4791\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87a7b0d541742648f4661b8eb4ebd15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "utokyo_2\n",
      "2647\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3faf4ba61323405684981e1d8cc8b68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=113.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "utokyo_1\n",
      "27419\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bbf1e3f8bc49479d7d48eeb336a24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=994.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nau_1\n",
      "1094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2429a46b01b042b080472e35fafec3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "reference_data = Path(\"/home/etdavid/Projects/1_research/3_wheat_counting/3-GWHD/kaggle-globalwheathead-winners/corrected\")\n",
    "output = Path(f\"/home/etdavid/Projects/1_research/3_wheat_counting/3-GWHD/biased-result/dungnb/submit\")\n",
    "\n",
    "output.mkdir(exist_ok=True,parents=True)\n",
    "\n",
    "for jsonp in reference_data.glob(\"*.json\"):\n",
    "    \n",
    "    sess = jsonp.with_suffix(\"\").name\n",
    "    print(sess)\n",
    "\n",
    "    annotations = json.load(jsonp.open())\n",
    "    print(len(annotations))\n",
    "    \n",
    "    # You can uncomment the line below if you have a complete COCO JSON\n",
    "    # annotations = annotations[\"annotations\"]\n",
    "    new_annotations = []\n",
    "    for ann in annotations:\n",
    "        ann[\"score\"] = 0.7\n",
    "        new_annotations.append(copy.copy(ann))\n",
    "\n",
    "    annotations = clean_json(new_annotations)\n",
    "    new_annotations = []\n",
    "    for ann in annotations:\n",
    "        try:\n",
    "            ann[\"score\"]\n",
    "            new_annotations.append(ann)\n",
    "        except:\n",
    "            del ann\n",
    "\n",
    "    for ann in new_annotations:\n",
    "        try:\n",
    "            ann[\"score\"]\n",
    "        except:\n",
    "            print(\"fail\")\n",
    "\n",
    "\n",
    "    (output / (f\"{jsonp.name}\")).write_text(json.dumps(new_annotations))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting\n",
    "\n",
    "- For the sake of simplicity, counting per image is equal to the number of annotated boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_countcsv(\n",
    "    sessions_path,\n",
    "    correspondance_path,\n",
    "    out_name,\n",
    "    ):\n",
    "    \n",
    "    count_csv = []\n",
    "    \n",
    "    correspondance = pd.read_csv(correspondance_path)\n",
    "    \n",
    "    correspondance_dict = {img_id:img_name for img_id,img_name in zip(correspondance[\"image_id\"].values,correspondance[\"image_name\"].values)}\n",
    "    for sessp in sessions_path:\n",
    "        annotations = json.load(sessp.open())\n",
    "        \n",
    "        if type(annotations) == dict:\n",
    "\n",
    "            annotations=annotations[\"annotations\"]\n",
    "            \n",
    "        session_name = sessp.with_suffix(\"\").name\n",
    "        \n",
    "        list_img = list(set([ann[\"image_id\"] for ann in annotations]))\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        for img_id in tqdm(list_img):\n",
    "            img_name = correspondance_dict[img_id]\n",
    "\n",
    "            boxes = np.array([np.array(ann[\"bbox\"]) for ann in annotations if ann[\"image_id\"] == img_id])\n",
    "            scores = np.array([np.array(ann[\"score\"]) for ann in annotations if ann[\"image_id\"] == img_id])\n",
    "\n",
    "            if len(boxes) > 1:\n",
    "\n",
    "                pick_1 = clean_boxes(boxes)\n",
    "                \n",
    "                count = len(boxes[pick_1])\n",
    "                score = np.mean(scores[pick_1])\n",
    "            else:\n",
    "                count  = 0\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "            count_csv.append([session_name, img_name, count, len(boxes),score])\n",
    "\n",
    "    count_df = pd.DataFrame(count_csv,columns=[\"session\",\"image_name\",\"count\",\"control_count\",\"score\"])\n",
    "\n",
    "    count_df.to_csv(out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a8cc4374254f2c80a91e8e4cd76083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5316a7c3784a729a3cef3e0ad92014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=113.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a76c5834944a3b8496b70336b074e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=994.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8cbb164b084e05bbe4fa797dda1954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sessions_path =list(reference_data.glob(\"*.json\"))\n",
    "\n",
    "generate_countcsv(sessions_path, \"correspondance.csv\",(output / \"count.csv\"))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capte-detection-3]",
   "language": "python",
   "name": "conda-env-capte-detection-3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
